{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9578452,"sourceType":"datasetVersion","datasetId":5839631}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\nfrom transformers import (BertTokenizer, BertForSequenceClassification, \n            AdamW, get_scheduler, DataCollatorWithPadding, AutoModel, \n            AutoTokenizer)\nfrom torch.utils.data import DataLoader\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import f1_score, classification_report\nfrom tqdm.auto import tqdm\n\nimport time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-27T17:06:46.030155Z","iopub.execute_input":"2024-11-27T17:06:46.030561Z","iopub.status.idle":"2024-11-27T17:06:46.035707Z","shell.execute_reply.started":"2024-11-27T17:06:46.030520Z","shell.execute_reply":"2024-11-27T17:06:46.034890Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/fhd-data/data/incidents_train.csv', index_col=0)\nvalid = pd.read_csv('/kaggle/input/fhd-data/data/incidents_dev.csv', index_col=0)\n\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n\ndef tokenize_function(examples):\n    return tokenizer(examples['title'], padding=True, truncation=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T16:34:21.956658Z","iopub.execute_input":"2024-11-27T16:34:21.957532Z","iopub.status.idle":"2024-11-27T16:34:22.288897Z","shell.execute_reply.started":"2024-11-27T16:34:21.957499Z","shell.execute_reply":"2024-11-27T16:34:22.287947Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def predict(texts, model_path, tokenizer_path=\"/kaggle/input/bert_tokenizer2/pytorch/default/1\"):\n    # Load the saved tokenizer and the saved model\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path) # moved to Auto\n    #model = BertForSequenceClassification.from_pretrained(model_path)\n    model = AutoModel.from_pretrained(model_path)\n    \n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    # Tokenize the input texts\n    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n    # Move inputs to the same device as the model\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Make predictions\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=-1)\n\n    return predictions.cpu().numpy()\n\ndef compute_score(hazards_true, products_true, hazards_pred, products_pred):\n  f1_hazards = f1_score(\n    hazards_true,\n    hazards_pred,\n    average='macro'\n  )\n\n  # compute f1 for products:\n  f1_products = f1_score(\n    products_true[hazards_pred == hazards_true],\n    products_pred[hazards_pred == hazards_true],\n    average='macro'\n  )\n\n  return (f1_hazards + f1_products) / 2.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T16:36:34.961053Z","iopub.execute_input":"2024-11-27T16:36:34.961425Z","iopub.status.idle":"2024-11-27T16:36:34.968856Z","shell.execute_reply.started":"2024-11-27T16:36:34.961395Z","shell.execute_reply":"2024-11-27T16:36:34.967970Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\nclass CustomLoss(nn.Module):\n    def __init__(self, whichloss='crossentropy', weights=None):\n        super(CustomLoss, self).__init__()\n        self.class_weights = weights\n\n    def forward(self, logits, labels):\n        # Apply sigmoid to logits to convert to probabilities\n        #probs = torch.sigmoid(logits)\n        #print(logits.shape, labels.shape)\n        # Calculate the weighted binary cross-entropy loss\n        #loss = -(\n        #    self.pos_weight * labels * torch.log(probs + 1e-9) + \n        #    (1 - labels) * torch.log(1 - probs + 1e-9)\n        #)\n        if whichloss == 'crossentropy':\n            loss = nn.functional.cross_entropy(logits, labels, weight=self.class_weights)\n        else:\n            print(\"NOT IMPLEMENTED ERROR\")\n        return loss\n\n# Instantiate your loss\ncustom_loss_fn = CustomLoss()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T17:02:06.906239Z","iopub.execute_input":"2024-11-27T17:02:06.906638Z","iopub.status.idle":"2024-11-27T17:02:06.912509Z","shell.execute_reply.started":"2024-11-27T17:02:06.906606Z","shell.execute_reply":"2024-11-27T17:02:06.911638Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"for label in tqdm(['hazard-category', 'product-category', 'hazard', 'product']):\n    label_encoder = LabelEncoder()\n    data['label'] = label_encoder.fit_transform(data[label])\n\n    # Data preprocessing\n    train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\n    train_dataset = Dataset.from_pandas(train_df)\n    test_dataset = Dataset.from_pandas(test_df)\n    train_dataset = train_dataset.map(tokenize_function, batched=True)\n    test_dataset = test_dataset.map(tokenize_function, batched=True)\n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding=True, max_length=16)\n    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n    test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8, collate_fn=data_collator)\n    test_dataloader = DataLoader(test_dataset, batch_size=8, collate_fn=data_collator)\n\n\n    model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(data[label].unique()) , output_hidden_states=False)\n    #model = AutoModel.from_pretrained('bert-base-uncased')\n    model.to('cuda')  # Move model to GPU if available\n\n    # training\n    optimizer = AdamW(model.parameters(), lr=5e-5)\n    num_epochs = 1\n    num_training_steps = num_epochs * len(train_dataloader)\n    lr_scheduler = get_scheduler(\n        name=\"linear\",\n        optimizer=optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_training_steps,\n    )\n\n    model.train()\n    progress_bar = tqdm(range(num_training_steps))\n    print(\"training starting ..\")\n    total_loss_list = []\n    \n    for epoch in tqdm(range(num_epochs), desc=\"TRAIN\"):\n        curr_ep_loss = 0\n        t1 = time.time()\n        for batch in train_dataloader:\n            inputs = {k: v.to('cuda') for k, v in batch.items() if k not in ['labels']}  # Move batch to GPU if available\n            labels = {k: v.to('cuda') for k, v in batch.items() if k in ['labels']}\n            outputs = model(**inputs)\n            #print(outputs.last_hidden_state.shape, outputs.pooler_output.shape)\n            #loss = outputs.loss\n            logits = outputs.logits  # Raw logits from model\n            # Compute custom loss\n            loss = custom_loss_fn(logits, **labels)\n            curr_ep_loss += loss.item()\n            \n            loss.backward()\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n            progress_bar.update(1)\n        t2 = time.time()\n        print(f\"Epoch {epoch + 1}, Loss: {curr_ep_loss:.4f} | Time : {(t2-t1):.4f} seconds\")\n\n    # assess model\n    model.eval()\n    total_predictions = []\n    with torch.no_grad():\n        for batch in test_dataloader:\n            inputs = {k: v.to('cuda') for k, v in batch.items() if k not in ['labels']}  # Move batch to GPU if available\n            labels = {k: v.to('cuda') for k, v in batch.items() if k in ['labels']}\n            outputs = model(**inputs)\n            predictions = torch.argmax(outputs.logits, dim=-1)\n            total_predictions.extend([p.item() for p in predictions])\n\n    predicted_labels = label_encoder.inverse_transform(total_predictions)\n    gold_labels = label_encoder.inverse_transform(test_df.label.values)\n    print(classification_report(gold_labels, predicted_labels, zero_division=0))\n\n    model.save_pretrained(f\"bert_{label}\")\n    break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T17:08:34.255902Z","iopub.execute_input":"2024-11-27T17:08:34.256392Z","iopub.status.idle":"2024-11-27T17:09:42.734277Z","shell.execute_reply.started":"2024-11-27T17:08:34.256347Z","shell.execute_reply":"2024-11-27T17:09:42.733123Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89a8623fa1a4481e9e7c8f01358a42cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4065 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3004203a9599454eb76389b920fbd4c7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1017 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47a8861c3fdb41999a1d0dcb4d9afb2a"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/509 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a12df5c8ffb44d6eb5e597ddb2320148"}},"metadata":{}},{"name":"stdout","text":"training starting ..\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"TRAIN:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5852d306b732446b8d4518f65804e471"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2855: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1, Loss: 415.8496 | Time : 62.6634 seconds\n                                precision    recall  f1-score   support\n\n                     allergens       0.82      0.94      0.88       377\n                    biological       0.83      0.93      0.88       339\n                      chemical       0.80      0.59      0.68        68\nfood additives and flavourings       0.00      0.00      0.00         5\n                foreign bodies       0.90      0.68      0.77       111\n                         fraud       0.80      0.57      0.67        68\n                     migration       0.00      0.00      0.00         1\n          organoleptic aspects       1.00      0.10      0.18        10\n                  other hazard       0.59      0.37      0.45        27\n              packaging defect       0.00      0.00      0.00        11\n\n                      accuracy                           0.82      1017\n                     macro avg       0.57      0.42      0.45      1017\n                  weighted avg       0.81      0.82      0.81      1017\n\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"{k:v for k,v in batch.items() if k not in ['labels']}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T16:43:28.017163Z","iopub.execute_input":"2024-11-27T16:43:28.017581Z","iopub.status.idle":"2024-11-27T16:43:28.030614Z","shell.execute_reply.started":"2024-11-27T16:43:28.017545Z","shell.execute_reply":"2024-11-27T16:43:28.029626Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[  101, 10651,  1015,  1024, 28390,  2080, 17722,  1037,  4989,  1997,\n           2785,  2121,  3688,  2138,  1997,  1996,  2825,  3739,  1997, 11840,\n           8411,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0],\n         [  101,  2088,  2740,  3688,  1010, 11775,  1012,  3314,  2035, 24395,\n           9131,  2006,  6151,  8586,  8017,  2098,  6501,  2035,  2121,  6914,\n           1999,  6892, 11263,  2884,  4487,  5397,  4588,   102,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0],\n         [  101,  8439,  2740, 13866,  2100,  5183,  1517, 13109,  8528, 19763,\n           2094,  2007, 22940,  1998, 22681,   102,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0],\n         [  101,  4086,  5416,  2072, 11112,  3695,  3540, 16027,  6501, 29593,\n           2015,  1011, 24792,  1010, 20710,  2361,  9766,  1998,  5810, 28126,\n           2015,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0],\n         [  101,  3679, 11203,  3314, 10758,  9131,  1997,  2413, 15307,  4014,\n           1009,  3389,  2243, 13675, 26607,  2015,  2349,  2000,  4022,  2740,\n           3891,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0],\n         [  101,  7734,  2806,  9440,  1010,  4297,  1012, 17722,  4031,  2138,\n           1997,  2825,  2740,  3891,   102,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0],\n         [  101,  2190,  9440,  4297,  1012,  3314,  2035, 24395,  9499,  2006,\n           6151,  8586,  8017,  2098, 27613,  1999,  8448,  4435, 13988,  2378,\n           9898,  1021, 19471,  1998,  8448,  4435, 13988,  2378,  9898,  2403,\n          19471,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0],\n         [  101,  4249,  8162,  3401,  2833,  3001,  1010,  4297,  1012, 17722,\n          12486,  1998, 22468,  3688,  2349,  2000, 28616, 23544,  2075,  1998,\n           6151,  8586,  8017,  2098,  2035,  2121, 21230,   102,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0]]),\n 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"##### PREDICTIONS #####\n\n# prediction ST1\nvalid_predictions_category = {}\nfor label in tqdm(['hazard-category', 'product-category']):\n  # Decode predictions back to string labels\n  label_encoder = LabelEncoder()\n  label_encoder.fit(data[label])\n  valid_predictions_category[label] = predict(valid.title.to_list(), f'bert_{label}')\n  valid_predictions_category[label] = label_encoder.inverse_transform(valid_predictions_category[label])\n\n# save predictions\nsolution = pd.DataFrame({'hazard-category': valid_predictions_category['hazard-category'], 'product-category': valid_predictions_category['product-category']})\nsolution.to_csv('/kaggle/working/submission_bert_st1.csv', index=False)\nprint(\"submission ST1 created!\")\n\n# prediction ST2\nvalid_predictions = {}\nfor label in tqdm(['hazard', 'product']):\n  # Decode predictions back to string labels\n  label_encoder = LabelEncoder()\n  label_encoder.fit(data[label])\n  valid_predictions[label] = predict(valid.title.to_list(), f'bert_{label}')\n  valid_predictions[label] = label_encoder.inverse_transform(valid_predictions[label])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:46:02.057950Z","iopub.status.idle":"2024-11-20T15:46:02.058405Z","shell.execute_reply.started":"2024-11-20T15:46:02.058163Z","shell.execute_reply":"2024-11-20T15:46:02.058190Z"}},"outputs":[],"execution_count":null}]}