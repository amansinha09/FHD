{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9578452,"sourceType":"datasetVersion","datasetId":5839631}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfrom datasets import Dataset\nfrom transformers import (BertTokenizer, BertForSequenceClassification,\n            AutoModelForSequenceClassification,\n            AdamW, get_scheduler, DataCollatorWithPadding, AutoModel, \n            AutoTokenizer)\nfrom torch.utils.data import DataLoader\nimport torch\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import f1_score, classification_report\nfrom tqdm.auto import tqdm\n\nfrom collections import Counter\nimport time","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-01T23:32:41.991721Z","iopub.execute_input":"2024-12-01T23:32:41.992078Z","iopub.status.idle":"2024-12-01T23:32:41.997467Z","shell.execute_reply.started":"2024-12-01T23:32:41.992045Z","shell.execute_reply":"2024-12-01T23:32:41.996572Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/fhd-data/data/incidents_train.csv', index_col=0)\nvalid = pd.read_csv('/kaggle/input/fhd-data/data/incidents_dev.csv', index_col=0)\n\ntokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n\ndef tokenize_function(examples):\n    return tokenizer(examples['title'], padding='max_length', max_length=512, truncation=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T23:32:42.881498Z","iopub.execute_input":"2024-12-01T23:32:42.881857Z","iopub.status.idle":"2024-12-01T23:32:43.400728Z","shell.execute_reply.started":"2024-12-01T23:32:42.881824Z","shell.execute_reply":"2024-12-01T23:32:43.399738Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"def predict(texts, model_path, tokenizer_path=\"/kaggle/input/bert_tokenizer2/pytorch/default/1\"):\n    # Load the saved tokenizer and the saved model\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_path) # moved to Auto\n    #model = BertForSequenceClassification.from_pretrained(model_path)\n    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n    \n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    # Tokenize the input texts\n    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n    # Move inputs to the same device as the model\n    inputs = {key: value.to(device) for key, value in inputs.items()}\n\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Make predictions\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        predictions = torch.argmax(logits, dim=-1)\n\n    return predictions.cpu().numpy()\n\ndef compute_score(hazards_true, products_true, hazards_pred, products_pred):\n  f1_hazards = f1_score(\n    hazards_true,\n    hazards_pred,\n    average='macro'\n  )\n\n  # compute f1 for products:\n  f1_products = f1_score(\n    products_true[hazards_pred == hazards_true],\n    products_pred[hazards_pred == hazards_true],\n    average='macro'\n  )\n\n  return (f1_hazards + f1_products) / 2.\n\ndef compute_class_weights_as_list(sample_counts):\n    \"\"\"\n    Compute class weights based on sample counts for each class.\n    \n    Args:\n        sample_counts (list or array): List of sample counts for each class.\n    \n    Returns:\n        list: List of class weights corresponding to each class.\n    \"\"\"\n    total_samples = sum(sample_counts)\n    num_classes = len(sample_counts)\n    \n    class_weights = torch.tensor([\n        total_samples / (num_classes * count)\n        for count in sample_counts\n    ]).to('cuda')\n    return class_weights","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T23:32:48.835777Z","iopub.execute_input":"2024-12-01T23:32:48.836127Z","iopub.status.idle":"2024-12-01T23:32:48.844724Z","shell.execute_reply.started":"2024-12-01T23:32:48.836097Z","shell.execute_reply":"2024-12-01T23:32:48.843766Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass FocalLoss(nn.Module):\n    def __init__(self, alpha=None, gamma=2.0, reduction='mean'):\n        \"\"\"\n        Args:\n            alpha: Tensor of shape (num_classes,) specifying weight for each class, or a scalar.\n                   If None, no class weights are applied.\n            gamma: Focusing parameter (default: 2.0).\n            reduction: Specifies the reduction to apply to the output: 'none' | 'mean' | 'sum'.\n        \"\"\"\n        super(FocalLoss, self).__init__()\n        self.alpha = alpha\n        self.gamma = gamma\n        self.reduction = reduction\n\n    def forward(self, logits, labels):\n        \"\"\"\n        Args:\n            logits: Tensor of shape (batch_size, num_classes). Raw model outputs.\n            labels: Tensor of shape (batch_size,). True class indices (not one-hot).\n        \"\"\"\n        # Compute softmax probabilities\n        probs = F.softmax(logits, dim=1)  # Shape: (batch_size, num_classes)\n\n        # Select the probabilities corresponding to the true class\n        # Shape: (batch_size,)\n        true_probs = probs[torch.arange(labels.size(0)), labels]\n\n        # Compute the focal loss\n        focal_weight = (1 - true_probs) ** self.gamma\n        log_probs = torch.log(true_probs + 1e-9)  # Add epsilon for numerical stability\n        loss = -focal_weight * log_probs\n\n        # Apply alpha weighting if provided\n        if self.alpha is not None:\n            if isinstance(self.alpha, torch.Tensor):  # Class-specific alpha\n                alpha_t = self.alpha[labels]\n            else:  # Scalar alpha\n                alpha_t = self.alpha\n            loss *= alpha_t\n\n        # Reduction: none | mean | sum\n        if self.reduction == 'mean':\n            return loss.mean()\n        elif self.reduction == 'sum':\n            return loss.sum()\n        else:  # 'none'\n            return loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-29T23:36:07.585328Z","iopub.execute_input":"2024-11-29T23:36:07.586022Z","iopub.status.idle":"2024-11-29T23:36:07.593507Z","shell.execute_reply.started":"2024-11-29T23:36:07.585988Z","shell.execute_reply":"2024-11-29T23:36:07.592549Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\nfrom torch import nn\n\nclass CustomLoss(nn.Module):\n    def __init__(self, whichloss='crossentropy', class_count=None):\n        super(CustomLoss, self).__init__()\n        self.class_count = torch.tensor(class_count).to('cuda')\n        self.class_weights = compute_class_weights_as_list(self.class_count)\n        self.whichloss = whichloss\n        self.num_classes = len(class_count)\n\n        # for focalloss\n        self.gamma = 2.0\n        self.alpha = None\n        self.reduction = 'mean'\n\n        # for classbalancedloss\n        self.beta = 0.99\n\n        # for equalizationloss\n        self.suppression_factor = 1.5\n\n        # for ldam loss\n        #self.cls_num_list = torch.tensor(cls_num_list, dtype=torch.float)\n        self.max_margin = 0.5\n        #self.weight = weight\n        #self.reduction = reduction\n        self.margins = self.max_margin / torch.sqrt(self.class_count)\n        self.margins = self.margins.to(torch.float)\n        \n\n    def forward(self, logits, labels):\n        if self.whichloss == 'softmax':\n            loss = nn.functional.cross_entropy(logits, labels, weight=None)\n        elif self.whichloss == 'wsoftmax':\n            loss = nn.functional.cross_entropy(logits, labels, weight=self.class_weights)\n        elif self.whichloss == 'focalloss':\n            # Compute softmax probabilities\n            probs = F.softmax(logits, dim=1)  # Shape: (batch_size, num_classes)\n            # Select the probabilities corresponding to the true class\n            # Shape: (batch_size,)\n            true_probs = probs[torch.arange(labels.size(0)), labels]\n            # Compute the focal loss\n            focal_weight = (1 - true_probs) ** self.gamma\n            log_probs = torch.log(true_probs + 1e-9)  # Add epsilon for numerical stability\n            loss = -focal_weight * log_probs\n            # Apply alpha weighting if provided\n            if self.alpha is not None:\n                if isinstance(self.alpha, torch.Tensor):  # Class-specific alpha\n                    alpha_t = self.alpha[labels]\n                else:  # Scalar alpha\n                    alpha_t = self.alpha\n                loss *= alpha_t\n            if self.reduction == 'mean':\n                return loss.mean()\n            elif self.reduction == 'sum':\n                return loss.sum()\n            else:  # 'none'\n                return loss\n        elif self.whichloss == 'classbalancedloss':\n            effective_num = 1.0 - torch.pow(self.beta, self.class_count)\n            weights = (1.0 - self.beta) / (effective_num + 1e-8)\n            weights = weights / weights.sum()  # Normalize weights\n    \n            # Convert targets to one-hot encoding\n            one_hot_targets = F.one_hot(labels, num_classes=self.num_classes).float()\n    \n            # Apply softmax to logits\n            probs = F.softmax(logits, dim=1)\n    \n            # Compute class-balanced cross-entropy loss\n            weighted_loss = -weights * one_hot_targets * torch.log(probs + 1e-8)\n            loss = weighted_loss.sum(dim=1)\n    \n            # Apply reduction\n            if self.reduction == 'mean':\n                loss = loss.mean()\n            elif self.reduction == 'sum':\n                loss = loss.sum()\n\n        elif self.whichloss == 'balancedsoftmax':\n            log_class_counts = torch.log(self.class_count.float() + 1e-8)  # Avoid log(0)\n            # Adjust logits by subtracting log class counts\n            adjusted_logits = logits - log_class_counts\n            # Compute the balanced softmax probabilities\n            balanced_probs = F.log_softmax(adjusted_logits, dim=1)\n            # Gather the log probabilities of the true classes\n            log_probs = balanced_probs[torch.arange(logits.size(0)), labels]\n            # Compute the loss\n            loss = -log_probs\n            # Apply reduction\n            if self.reduction == 'mean':\n                loss = loss.mean()\n            elif self.reduction == 'sum':\n                loss = loss.sum()\n        elif self.whichloss == 'equalizationloss':\n            one_hot_targets = F.one_hot(labels, num_classes=self.num_classes).float()\n    \n            # Compute probabilities with softmax\n            probs = F.softmax(logits, dim=1)\n    \n            # Suppression weights for negative samples\n            effective_num = torch.pow(self.class_count.float(), self.suppression_factor)\n            weights = (1.0 / effective_num).to(logits.device)\n    \n            # Broadcast weights to match batch size and one-hot targets\n            weight_matrix = one_hot_targets + (1 - one_hot_targets) * weights.unsqueeze(0)\n    \n            # Compute weighted cross-entropy loss\n            ce_loss = -one_hot_targets * torch.log(probs + 1e-8)\n            suppressed_loss = ce_loss * weight_matrix\n    \n            # Sum over classes and apply reduction\n            loss = suppressed_loss.sum(dim=1)\n            if self.reduction == 'mean':\n                loss = loss.mean()\n            elif self.reduction == 'sum':\n                loss = loss.sum()\n        elif self.whichloss == 'ldamloss':\n            batch_size, num_classes = logits.size()\n            \n            # Create a margin matrix\n            margins = torch.zeros_like(logits)\n            margins[torch.arange(batch_size), labels] = self.margins[labels]\n            \n            # Adjust logits with the margin\n            adjusted_logits = logits - margins\n            \n            # Compute cross-entropy loss\n            loss = F.cross_entropy(adjusted_logits, labels, weight=self.class_weights, reduction=self.reduction)\n        else:\n            print(\"NOT IMPLEMENTED ERROR\")\n        return loss\n\n# Instantiate your loss\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T23:32:57.546872Z","iopub.execute_input":"2024-12-01T23:32:57.547186Z","iopub.status.idle":"2024-12-01T23:32:57.562969Z","shell.execute_reply.started":"2024-12-01T23:32:57.547158Z","shell.execute_reply":"2024-12-01T23:32:57.562042Z"}},"outputs":[],"execution_count":66},{"cell_type":"code","source":"for label in tqdm(['hazard-category', 'product-category', 'hazard', 'product']):\n    label_encoder = LabelEncoder()\n    data[f'{label}_label'] = label_encoder.fit_transform(data[label])\n\nfor label in tqdm(['hazard-category', 'product-category', 'hazard', 'product']):\n    #label_encoder = LabelEncoder()\n    data['label'] = data[f'{label}_label']\n\n    # Data preprocessing\n    train_df, test_df = train_test_split(data, test_size=0.2, random_state=42)\n    train_df, test_df = train_df.iloc[:100,:], test_df\n    \n    train_dataset = Dataset.from_pandas(train_df)\n    test_dataset = Dataset.from_pandas(test_df)\n    train_dataset = train_dataset.map(tokenize_function, batched=True)\n    test_dataset = test_dataset.map(tokenize_function, batched=True)\n    print(train_dataset)\n    \n    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, padding='max_length', max_length=512)\n    train_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n    test_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n    train_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=8, collate_fn=data_collator)\n    test_dataloader = DataLoader(test_dataset, batch_size=8, collate_fn=data_collator)\n\n    model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(data[label].unique()) , output_hidden_states=False)\n    #model = AutoModel.from_pretrained('bert-base-uncased')\n    model.to('cuda')  # Move model to GPU if available\n\n    # training\n    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n    num_epochs = 4\n    num_training_steps = num_epochs * len(train_dataloader)\n    lr_scheduler = get_scheduler(\n        name=\"linear\",\n        optimizer=optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_training_steps,\n    )\n    dd=dict(Counter(data['label'].values))\n    class_count = [dd[i] for i in range(len(dd))]\n    \n    custom_loss_fn = CustomLoss(whichloss = 'ldamloss',\n                               class_count = class_count)\n\n    model.train()\n    progress_bar = tqdm(range(num_training_steps))\n    print(\"training starting ..\")\n    total_loss_list = []\n    \n    for epoch in tqdm(range(num_epochs), desc=\"TRAIN\"):\n        curr_ep_loss = 0\n        t1 = time.time()\n        for batch in train_dataloader:\n            inputs = {k: v.to('cuda') for k, v in batch.items() if k not in ['labels']}  # Move batch to GPU if available\n            labels = {k: v.to('cuda') for k, v in batch.items() if k in ['labels']}\n            outputs = model(**inputs)\n            #print(outputs.last_hidden_state.shape, outputs.pooler_output.shape)\n            #loss = outputs.loss\n            logits = outputs.logits  # Raw logits from model\n            # Compute custom loss\n            loss = custom_loss_fn(logits, **labels)\n            curr_ep_loss += loss.item()\n            \n            loss.backward()\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n            progress_bar.update(1)\n        t2 = time.time()\n        print(f\"Epoch {epoch + 1}, Loss: {curr_ep_loss:.4f} | Time : {(t2-t1):.4f} seconds\")\n\n    # assess model\n    model.eval()\n    total_predictions = []\n    with torch.no_grad():\n        for batch in test_dataloader:\n            inputs = {k: v.to('cuda') for k, v in batch.items() if k not in ['labels']}  # Move batch to GPU if available\n            labels = {k: v.to('cuda') for k, v in batch.items() if k in ['labels']}\n            outputs = model(**inputs)\n            predictions = torch.argmax(outputs.logits, dim=-1)\n            total_predictions.extend([p.item() for p in predictions])\n\n    predicted_labels = label_encoder.inverse_transform(total_predictions)\n    gold_labels = label_encoder.inverse_transform(test_df.label.values)\n    print(classification_report(gold_labels, predicted_labels, zero_division=0))\n\n    model.save_pretrained(f\"bert_{label}\")\n    break\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T23:35:08.182569Z","iopub.execute_input":"2024-12-01T23:35:08.183518Z","iopub.status.idle":"2024-12-01T23:36:29.606182Z","shell.execute_reply.started":"2024-12-01T23:35:08.183478Z","shell.execute_reply":"2024-12-01T23:36:29.604907Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abdf7c408327495aafb2937babeec60c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0aade8ce62b461cabc1a8bc00b6abce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfc48470d02d4ce39ea547b963d7f69d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1017 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"397bb601669f4edb9a231e131ba4946b"}},"metadata":{}},{"name":"stdout","text":"Dataset({\n    features: ['year', 'month', 'day', 'country', 'title', 'text', 'hazard-category', 'product-category', 'hazard', 'product', 'hazard-category_label', 'product-category_label', 'hazard_label', 'product_label', 'label', '__index_level_0__', 'input_ids', 'token_type_ids', 'attention_mask'],\n    num_rows: 100\n})\n","output_type":"stream"},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/52 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8339c2361f3140acbec05b96587d21e4"}},"metadata":{}},{"name":"stdout","text":"training starting ..\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"TRAIN:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fa4e54cda7140489cd43f7e791069f9"}},"metadata":{}},{"name":"stdout","text":"Epoch 1, Loss: 29.9738 | Time : 10.2226 seconds\nEpoch 2, Loss: 26.2441 | Time : 10.8286 seconds\nEpoch 3, Loss: 24.6166 | Time : 11.1994 seconds\nEpoch 4, Loss: 22.8788 | Time : 11.2598 seconds\n                                     precision    recall  f1-score   support\n\n             Catfishes (freshwater)       0.44      0.89      0.59       377\n                    Dried pork meat       0.86      0.09      0.16       339\n              Fishes not identified       0.00      0.00      0.00        68\n                 Groupers (generic)       0.00      0.00      0.00         5\n           Not classified pork meat       0.00      0.00      0.00       111\n         Pangas catfishes (generic)       0.18      0.56      0.27        68\nPrecooked cooked pork meat products       0.00      0.00      0.00         1\n Torpedo-shaped catfishes (generic)       0.00      0.00      0.00        10\n                      Veggie Burger       0.00      0.00      0.00        27\n                    adobo seasoning       0.00      0.00      0.00        11\n\n                           accuracy                           0.40      1017\n                          macro avg       0.15      0.15      0.10      1017\n                       weighted avg       0.46      0.40      0.29      1017\n\n","output_type":"stream"}],"execution_count":68},{"cell_type":"code","source":"{k:v for k,v in batch.items() if k not in ['labels']}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-27T16:43:28.017163Z","iopub.execute_input":"2024-11-27T16:43:28.017581Z","iopub.status.idle":"2024-11-27T16:43:28.030614Z","shell.execute_reply.started":"2024-11-27T16:43:28.017545Z","shell.execute_reply":"2024-11-27T16:43:28.029626Z"},"jupyter":{"source_hidden":true,"outputs_hidden":true},"collapsed":true},"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"{'input_ids': tensor([[  101, 10651,  1015,  1024, 28390,  2080, 17722,  1037,  4989,  1997,\n           2785,  2121,  3688,  2138,  1997,  1996,  2825,  3739,  1997, 11840,\n           8411,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0],\n         [  101,  2088,  2740,  3688,  1010, 11775,  1012,  3314,  2035, 24395,\n           9131,  2006,  6151,  8586,  8017,  2098,  6501,  2035,  2121,  6914,\n           1999,  6892, 11263,  2884,  4487,  5397,  4588,   102,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0],\n         [  101,  8439,  2740, 13866,  2100,  5183,  1517, 13109,  8528, 19763,\n           2094,  2007, 22940,  1998, 22681,   102,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0],\n         [  101,  4086,  5416,  2072, 11112,  3695,  3540, 16027,  6501, 29593,\n           2015,  1011, 24792,  1010, 20710,  2361,  9766,  1998,  5810, 28126,\n           2015,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0],\n         [  101,  3679, 11203,  3314, 10758,  9131,  1997,  2413, 15307,  4014,\n           1009,  3389,  2243, 13675, 26607,  2015,  2349,  2000,  4022,  2740,\n           3891,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0],\n         [  101,  7734,  2806,  9440,  1010,  4297,  1012, 17722,  4031,  2138,\n           1997,  2825,  2740,  3891,   102,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0],\n         [  101,  2190,  9440,  4297,  1012,  3314,  2035, 24395,  9499,  2006,\n           6151,  8586,  8017,  2098, 27613,  1999,  8448,  4435, 13988,  2378,\n           9898,  1021, 19471,  1998,  8448,  4435, 13988,  2378,  9898,  2403,\n          19471,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0],\n         [  101,  4249,  8162,  3401,  2833,  3001,  1010,  4297,  1012, 17722,\n          12486,  1998, 22468,  3688,  2349,  2000, 28616, 23544,  2075,  1998,\n           6151,  8586,  8017,  2098,  2035,  2121, 21230,   102,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n              0,     0,     0,     0,     0,     0,     0]]),\n 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n          1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"},"metadata":{}}],"execution_count":22},{"cell_type":"code","source":"##### PREDICTIONS #####\n\n# prediction ST1\nvalid_predictions_category = {}\nfor label in tqdm(['hazard-category', 'product-category']):\n  # Decode predictions back to string labels\n  label_encoder = LabelEncoder()\n  label_encoder.fit(data[label])\n  valid_predictions_category[label] = predict(valid.title.to_list(), f'bert_{label}')\n  valid_predictions_category[label] = label_encoder.inverse_transform(valid_predictions_category[label])\n\n# save predictions\nsolution = pd.DataFrame({'hazard-category': valid_predictions_category['hazard-category'], 'product-category': valid_predictions_category['product-category']})\nsolution.to_csv('/kaggle/working/submission_bert_st1.csv', index=False)\nprint(\"submission ST1 created!\")\n\n# prediction ST2\nvalid_predictions = {}\nfor label in tqdm(['hazard', 'product']):\n  # Decode predictions back to string labels\n  label_encoder = LabelEncoder()\n  label_encoder.fit(data[label])\n  valid_predictions[label] = predict(valid.title.to_list(), f'bert_{label}')\n  valid_predictions[label] = label_encoder.inverse_transform(valid_predictions[label])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-20T15:46:02.057950Z","iopub.status.idle":"2024-11-20T15:46:02.058405Z","shell.execute_reply.started":"2024-11-20T15:46:02.058163Z","shell.execute_reply":"2024-11-20T15:46:02.058190Z"}},"outputs":[],"execution_count":null}]}